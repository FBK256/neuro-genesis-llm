# Neuro Genesis LLM

![python](https://img.shields.io/badge/python-3.9%2B-blue?logo=python)
![pytorch](https://img.shields.io/badge/PyTorch-2.x-%23EE4C2C?logo=pytorch)
![license](https://img.shields.io/badge/license-Apache%202.0-green)

**Neuro Genesis LLM** ã¯ã€å‹•çš„ãƒ“ãƒ³åˆ†å‰²ã«ã‚ˆã‚‹é©å¿œåŸ‹ã‚è¾¼ã¿ (adaptive embedding) ã¨  
DeepSpeed ZeRO-3 ã«æœ€é©åŒ–ã—ãŸå¤§è¦æ¨¡ Transformer è¨€èªãƒ¢ãƒ‡ãƒ«ã®äº‹å‰å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚  
åŒæ¢±ã® **NeuroTokenizer** ãŒ BPE ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®å­¦ç¿’ï¼ä¿å­˜ã‚’è‡ªå‹•åŒ–ã—ã€  
`neuro_genesis.py` ã ã‘ã§ *Tokenizer â–¶ Pre-training â–¶ Inference* ã¾ã§å®Œçµã—ã¾ã™ã€‚

---

## âœ¨ ä¸»ãªç‰¹å¾´

| æ©Ÿèƒ½ | æ¦‚è¦ |
|------|------|
| **Dynamic Bins & Adaptive Embedding** | å˜èªé »åº¦ã«å¿œã˜ã¦ãƒ™ã‚¯ãƒˆãƒ«æ¬¡å…ƒã‚’å‹•çš„ç¸®å°ã— GPU ãƒ¡ãƒ¢ãƒªã‚’åœ§ç¸® |
| **Multi-layer Transformer** | 8 å±¤ãƒ»8 ãƒ˜ãƒƒãƒ‰ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ)ã€‚è¨­å®šã‚¯ãƒ©ã‚¹ã§ç°¡å˜å¤‰æ›´ |
| **DeepSpeed ZeRO-3** | ã‚®ã‚¬ãƒã‚¤ãƒˆç´šãƒ¢ãƒ‡ãƒ«ã‚’ 1 ï½ 2 GPU ã§å­¦ç¿’å¯èƒ½ |
| **Safetensors ä¿å­˜** | `safetensors` å½¢å¼ã§é«˜é€Ÿã‹ã¤å®‰å…¨ã«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå‡ºåŠ› |
| **TensorBoard ãƒ­ã‚®ãƒ³ã‚°** | æå¤±ã‚„å­¦ç¿’é€Ÿåº¦ã‚’å³æ™‚å¯è¦–åŒ– |
| **ç°¡æ˜“æ¤œç´¢ãƒ¡ãƒ¢ãƒª (state_memory)** | æ¨è«–æ™‚å¹³å‡åŸ‹ã‚è¾¼ã¿ã‚’ FIFO ä¿å­˜ã—ã€å¤–éƒ¨æ¤œç´¢ã¨ã®æ‹¡å¼µå®¹æ˜“ |

---

## ğŸ“¦ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
git clone https://github.com/<your-account>/neuro-genesis-llm.git
cd neuro-genesis-llm
pip install -r requirements.txt

requirements.txt ä¾‹:
torch>=2.0
deepspeed
tokenizers
safetensors
tensorboardX

ğŸš€ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ
1. ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼å­¦ç¿’
from neuro_genesis import NeuroTokenizer
tok = NeuroTokenizer("config/tokenizer.json")
tok.train(["data/general.json", "data/code.json"])

2. äº‹å‰å­¦ç¿’
from neuro_genesis import NeuroPretrainer
trainer = NeuroPretrainer("config/model.json")
trainer.train(
    dataset_paths=["data/general.tok", "data/code.tok"],
    epochs=10
)

3. æ¨è«–
import torch
from neuro_genesis import NeuroGenesisModel, NeuroTokenizer

tok = NeuroTokenizer("tokenizer_config.json")
model = NeuroGenesisModel.from_pretrained("neuro_genesis_final").eval()

prompt = "Q: å¯Œå£«å±±ã®æ¨™é«˜ã¯ï¼Ÿ\nA:"
ids = tok.tokenizer.encode(prompt).ids
with torch.no_grad():
    logits = model(torch.tensor([ids]))
print(tok.tokenizer.decode(torch.argmax(logits, dim=-1)[0].tolist()))

ğŸ—‚ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹æˆ (æ¨å¥¨)
neuro-genesis-llm/
â”œâ”€â”€ neuro_genesis.py          # ãƒ¡ã‚¤ãƒ³å®Ÿè£…
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ model.json            # ãƒ¢ãƒ‡ãƒ«ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
â”‚   â””â”€â”€ tokenizer.json        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼è¨­å®š
â”œâ”€â”€ data/                     # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
â”œâ”€â”€ checkpoints/              # DeepSpeed ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
